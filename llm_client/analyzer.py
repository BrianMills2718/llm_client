"""Self-improvement analyzer for task graph runs.

Reads experiment JSONL logs, classifies issues into 8 categories, generates
improvement proposals with risk classification, and maintains model_floors.json
for cumulative learning.

Usage:
    from llm_client.analyzer import analyze_run, analyze_history

    # After a graph run
    proposals = analyze_run(report)

    # Standalone analysis of historical logs
    proposals = analyze_history(experiment_log="path/to/experiments.jsonl")

See docs/TASK_GRAPH_DESIGN.md for the full failure taxonomy and proposal format.
"""

from __future__ import annotations

import json
import logging
import sqlite3
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from statistics import mean, variance
from typing import Any

from pydantic import BaseModel

from llm_client.difficulty import (
    get_model_for_difficulty,
    load_model_floors,
    save_model_floors,
)
from llm_client.git_utils import PROMPT_CHANGE, classify_diff_files, get_diff_files
from llm_client.task_graph import ExecutionReport, ExperimentRecord, TaskStatus
from llm_client.validators import ValidationResult, run_validators

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data models
# ---------------------------------------------------------------------------


class IssueCategory:
    """8-category failure taxonomy."""

    MODEL_OVERKILL = "MODEL_OVERKILL"
    MODEL_UNDERKILL = "MODEL_UNDERKILL"
    PROMPT_DRIFT = "PROMPT_DRIFT"
    VALIDATION_NOISE = "VALIDATION_NOISE"
    TOOL_GAP = "TOOL_GAP"
    STUCK_LOOP = "STUCK_LOOP"
    DATA_QUALITY = "DATA_QUALITY"
    MEASUREMENT_ERROR = "MEASUREMENT_ERROR"


class Proposal(BaseModel):
    """An improvement proposal generated by the analyzer."""

    proposal_id: str
    timestamp: str
    category: str
    task_id: str
    graph_id: str
    evidence: dict[str, Any]
    risk: str  # "low" | "medium" | "high"
    action: str
    auto_apply: bool
    applied: bool = False
    result: str | None = None


class AnalysisReport(BaseModel):
    """Summary of an analysis run."""

    experiments_analyzed: int
    tasks_analyzed: int
    proposals: list[Proposal]
    floors_updated: dict[str, dict[str, Any]]
    reliability_checks: list[dict[str, Any]]
    score_proposals_count: int = 0


# ---------------------------------------------------------------------------
# Experiment log reading
# ---------------------------------------------------------------------------


def _load_experiments(path: Path) -> list[ExperimentRecord]:
    """Load experiment records from JSONL file."""
    if not path.exists():
        return []
    records: list[ExperimentRecord] = []
    for line in path.read_text().splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            records.append(ExperimentRecord(**json.loads(line)))
        except Exception:
            logger.debug("Skipping malformed experiment record", exc_info=True)
    return records


def _group_by_task(
    records: list[ExperimentRecord],
) -> dict[str, list[ExperimentRecord]]:
    """Group experiments by task_id, ordered by timestamp."""
    groups: dict[str, list[ExperimentRecord]] = defaultdict(list)
    for r in records:
        groups[r.task_id].append(r)
    for task_id in groups:
        groups[task_id].sort(key=lambda r: r.timestamp)
    return dict(groups)


# ---------------------------------------------------------------------------
# Issue classifiers
# ---------------------------------------------------------------------------

# Minimum consecutive successes before proposing a downgrade
_OVERKILL_THRESHOLD = 5


def _check_model_overkill(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks that consistently succeed — candidate for cheaper model."""
    recent = runs[-_OVERKILL_THRESHOLD:]
    if len(recent) < _OVERKILL_THRESHOLD:
        return None

    # All must be successes at the same tier
    if not all(r.outcome == "confirmed" for r in recent):
        return None

    current_tier = recent[-1].difficulty
    if current_tier <= 0:
        return None  # Can't go lower than 0

    proposed_tier = current_tier - 1
    try:
        proposed_model = get_model_for_difficulty(proposed_tier, available_only=False)
    except (ValueError, RuntimeError):
        return None

    avg_cost = sum(
        r.result.get("cost_usd", 0) for r in recent
    ) / len(recent)

    graph_id = recent[-1].run_id.rsplit("_", 1)[0] if "_" in recent[-1].run_id else recent[-1].run_id

    return Proposal(
        proposal_id=_make_id("overkill", task_id),
        timestamp=_now(),
        category=IssueCategory.MODEL_OVERKILL,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "runs_analyzed": len(recent),
            "success_rate_at_current_tier": 1.0,
            "current_tier": current_tier,
            "current_model": recent[-1].model_selected,
            "proposed_tier": proposed_tier,
            "proposed_model": proposed_model,
            "estimated_savings_per_run": round(avg_cost * 0.5, 6),
        },
        risk="low",
        action="downgrade_model",
        auto_apply=True,
    )


def _check_model_underkill(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks that failed at current tier — need a stronger model."""
    if not runs:
        return None
    latest = runs[-1]
    if latest.outcome != "hypothesis_rejected":
        return None

    current_tier = latest.difficulty
    proposed_tier = current_tier + 1
    if proposed_tier > 4:
        return None  # Already at max

    try:
        proposed_model = get_model_for_difficulty(proposed_tier, available_only=False)
    except (ValueError, RuntimeError):
        return None

    graph_id = latest.run_id.rsplit("_", 1)[0] if "_" in latest.run_id else latest.run_id

    return Proposal(
        proposal_id=_make_id("underkill", task_id),
        timestamp=_now(),
        category=IssueCategory.MODEL_UNDERKILL,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "current_tier": current_tier,
            "current_model": latest.model_selected,
            "proposed_tier": proposed_tier,
            "proposed_model": proposed_model,
            "failure_reason": latest.result.get("validation_results", []),
        },
        risk="high",
        action="upgrade_model",
        auto_apply=False,  # Upgrades need human approval
    )


def _check_stuck_loop(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks that timed out or produced errors (not validation failures)."""
    if not runs:
        return None
    latest = runs[-1]
    if latest.outcome != "error":
        return None

    # Count consecutive errors
    error_streak = 0
    for r in reversed(runs):
        if r.outcome == "error":
            error_streak += 1
        else:
            break

    if error_streak < 2:
        return None  # Single error could be transient

    graph_id = latest.run_id.rsplit("_", 1)[0] if "_" in latest.run_id else latest.run_id

    return Proposal(
        proposal_id=_make_id("stuck", task_id),
        timestamp=_now(),
        category=IssueCategory.STUCK_LOOP,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "consecutive_errors": error_streak,
            "latest_error": str(latest.result.get("validation_results", "unknown")),
            "duration_s": latest.result.get("duration_s", 0),
        },
        risk="medium",
        action="reduce_timeout_or_add_stop_conditions",
        auto_apply=False,
    )


def _check_validation_noise(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks with inconsistent validation results across runs."""
    if len(runs) < 4:
        return None

    recent = runs[-4:]
    outcomes = [r.outcome for r in recent]

    # Look for alternating pass/fail pattern: at least 2 passes and 2 fails
    passes = outcomes.count("confirmed")
    fails = outcomes.count("hypothesis_rejected")

    if passes < 2 or fails < 2:
        return None

    graph_id = recent[-1].run_id.rsplit("_", 1)[0] if "_" in recent[-1].run_id else recent[-1].run_id

    return Proposal(
        proposal_id=_make_id("noise", task_id),
        timestamp=_now(),
        category=IssueCategory.VALIDATION_NOISE,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "runs_analyzed": len(recent),
            "pass_count": passes,
            "fail_count": fails,
            "pattern": outcomes,
        },
        risk="medium",
        action="investigate_validator_stability",
        auto_apply=False,
    )


# ---------------------------------------------------------------------------
# Score-based classifiers (read from task_scores SQLite table)
# ---------------------------------------------------------------------------

_DEFAULT_DB_PATH = Path.home() / "projects" / "data" / "llm_observability.db"


def _load_scores_by_task(
    db_path: str | Path,
    project: str | None = None,
) -> dict[str, list[dict[str, Any]]]:
    """Read task_scores from SQLite, group by task, ordered by timestamp.

    Returns {} if DB doesn't exist or has no task_scores table.
    """
    db_path = Path(db_path)
    if not db_path.exists():
        return {}
    try:
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        query = "SELECT * FROM task_scores ORDER BY timestamp"
        params: list[Any] = []
        if project:
            query = "SELECT * FROM task_scores WHERE project = ? ORDER BY timestamp"
            params = [project]
        rows = conn.execute(query, params).fetchall()
        conn.close()
    except Exception:
        logger.debug("_load_scores_by_task failed", exc_info=True)
        return {}

    groups: dict[str, list[dict[str, Any]]] = defaultdict(list)
    for row in rows:
        d = dict(row)
        task = d.get("task") or "unknown"
        groups[task].append(d)
    return dict(groups)


def _check_prompt_drift(
    task_id: str,
    scores: list[dict[str, Any]],
    cwd: str | None = None,
) -> Proposal | None:
    """Detect score drops correlated with prompt file changes.

    Groups scores by git_commit, compares mean scores between consecutive
    commits. If drop > 0.15 AND git diff contains prompt files → proposal.
    Requires >=2 scores per commit to avoid noise.
    """
    # Group by git_commit
    by_commit: dict[str, list[float]] = defaultdict(list)
    commit_order: list[str] = []
    for s in scores:
        commit = s.get("git_commit")
        if not commit:
            continue
        if commit not in by_commit:
            commit_order.append(commit)
        by_commit[commit].append(s["overall_score"])

    if len(commit_order) < 2:
        return None

    # Check consecutive pairs
    for i in range(len(commit_order) - 1):
        prev_commit = commit_order[i]
        curr_commit = commit_order[i + 1]
        prev_scores = by_commit[prev_commit]
        curr_scores = by_commit[curr_commit]

        if len(prev_scores) < 2 or len(curr_scores) < 2:
            continue

        prev_mean = mean(prev_scores)
        curr_mean = mean(curr_scores)
        drop = prev_mean - curr_mean

        if drop <= 0.15:
            continue

        # Check if diff contains prompt files
        diff_files = get_diff_files(prev_commit, curr_commit, cwd=cwd)
        categories = classify_diff_files(diff_files)
        if PROMPT_CHANGE not in categories:
            continue

        prompt_files = [
            f for f in diff_files
            if "prompts" in f.split("/") or (f.endswith(".yaml") and "prompts" in f)
        ]

        return Proposal(
            proposal_id=_make_id("drift", task_id),
            timestamp=_now(),
            category=IssueCategory.PROMPT_DRIFT,
            task_id=task_id,
            graph_id="scores",
            evidence={
                "prev_commit": prev_commit,
                "curr_commit": curr_commit,
                "prev_mean_score": round(prev_mean, 4),
                "curr_mean_score": round(curr_mean, 4),
                "score_drop": round(drop, 4),
                "changed_prompt_files": prompt_files,
            },
            risk="medium",
            action="review_prompt_change",
            auto_apply=False,
        )

    return None


def _check_data_quality(
    task_id: str,
    scores: list[dict[str, Any]],
) -> Proposal | None:
    """Detect dimensions consistently scoring <= 2/5.

    Checks last N scores for any dimension with 3+ consecutive low scores.
    """
    if len(scores) < 3:
        return None

    recent = scores[-10:]  # Look at last 10

    # Parse dimensions from each score
    dim_series: dict[str, list[int]] = defaultdict(list)
    for s in recent:
        dims_raw = s.get("dimensions")
        if not dims_raw:
            continue
        if isinstance(dims_raw, str):
            try:
                dims = json.loads(dims_raw)
            except (json.JSONDecodeError, TypeError):
                continue
        else:
            dims = dims_raw
        for dim_name, score_val in dims.items():
            if isinstance(score_val, (int, float)):
                dim_series[dim_name].append(int(score_val))

    weak_dims: dict[str, float] = {}
    for dim_name, values in dim_series.items():
        if len(values) < 3:
            continue
        # Check for 3+ consecutive low scores at end
        consecutive_low = 0
        for v in reversed(values):
            if v <= 2:
                consecutive_low += 1
            else:
                break
        if consecutive_low >= 3:
            weak_dims[dim_name] = round(mean(values[-consecutive_low:]), 2)

    if not weak_dims:
        return None

    return Proposal(
        proposal_id=_make_id("dataqual", task_id),
        timestamp=_now(),
        category=IssueCategory.DATA_QUALITY,
        task_id=task_id,
        graph_id="scores",
        evidence={
            "weak_dimensions": weak_dims,
            "scores_analyzed": len(recent),
        },
        risk="medium",
        action="investigate_weak_dimension",
        auto_apply=False,
    )


def _check_measurement_error(
    task_id: str,
    scores: list[dict[str, Any]],
) -> Proposal | None:
    """Detect unreliable judge — high variance within same (rubric, git_commit) group.

    If variance > 0.3 within a group of 3+ scores → judge is unreliable.
    """
    # Group by (rubric, git_commit)
    groups: dict[tuple[str, str], list[float]] = defaultdict(list)
    for s in scores:
        rubric = s.get("rubric", "")
        commit = s.get("git_commit", "")
        if not rubric:
            continue
        groups[(rubric, commit)].append(s["overall_score"])

    for (rubric, commit), values in groups.items():
        if len(values) < 3:
            continue
        var = variance(values)
        if var > 0.3:
            return Proposal(
                proposal_id=_make_id("measerr", task_id),
                timestamp=_now(),
                category=IssueCategory.MEASUREMENT_ERROR,
                task_id=task_id,
                graph_id="scores",
                evidence={
                    "rubric": rubric,
                    "git_commit": commit or None,
                    "scores": [round(v, 4) for v in values],
                    "variance": round(var, 4),
                },
                risk="high",
                action="recalibrate_rubric",
                auto_apply=False,
            )

    return None


def _generate_score_proposals(
    scores_by_task: dict[str, list[dict[str, Any]]],
    cwd: str | None = None,
) -> list[Proposal]:
    """Run all score classifiers across all tasks, collect proposals."""
    proposals: list[Proposal] = []
    for task_id, scores in scores_by_task.items():
        for checker in (
            lambda tid, s: _check_prompt_drift(tid, s, cwd=cwd),
            _check_data_quality,
            _check_measurement_error,
        ):
            proposal = checker(task_id, scores)
            if proposal is not None:
                proposals.append(proposal)
    return proposals


# ---------------------------------------------------------------------------
# Scorer reliability check
# ---------------------------------------------------------------------------


def check_scorer_reliability(
    task_id: str,
    validator_configs: list[dict[str, Any]],
) -> dict[str, Any]:
    """Re-run validators on existing outputs to check measurement stability.

    Args:
        task_id: The task being checked.
        validator_configs: The validator configs from the task definition.

    Returns:
        Dict with task_id, stable (bool), results, and any discrepancies.
    """
    if not validator_configs:
        return {"task_id": task_id, "stable": True, "results": [], "note": "no validators"}

    # Run validators twice
    run1 = run_validators(validator_configs)
    run2 = run_validators(validator_configs)

    discrepancies: list[dict[str, Any]] = []
    for i, (r1, r2) in enumerate(zip(run1, run2)):
        if r1.passed != r2.passed:
            discrepancies.append({
                "validator_index": i,
                "type": r1.type,
                "run1_passed": r1.passed,
                "run2_passed": r2.passed,
                "run1_value": r1.value,
                "run2_value": r2.value,
            })

    return {
        "task_id": task_id,
        "stable": len(discrepancies) == 0,
        "results": [r.model_dump() for r in run1],
        "discrepancies": discrepancies,
    }


# ---------------------------------------------------------------------------
# Model floors maintenance
# ---------------------------------------------------------------------------


def _update_floors(
    experiments: dict[str, list[ExperimentRecord]],
    floors_path: str | Path | None = None,
) -> dict[str, dict[str, Any]]:
    """Update model_floors.json based on experiment results.

    Returns the updated floors dict.
    """
    floors = load_model_floors(floors_path)
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    for task_id, runs in experiments.items():
        if not runs:
            continue

        successes = [r for r in runs if r.outcome == "confirmed"]
        failures = [r for r in runs if r.outcome in ("hypothesis_rejected", "error")]

        if task_id not in floors:
            floors[task_id] = {
                "floor": runs[-1].difficulty,
                "ceiling": runs[-1].difficulty,
                "last_tested": today,
                "runs": len(runs),
            }
        else:
            floors[task_id]["runs"] = floors[task_id].get("runs", 0) + len(runs)
            floors[task_id]["last_tested"] = today

        # Lower floor if we have successes at a lower tier
        if successes:
            min_success_tier = min(r.difficulty for r in successes)
            current_floor = floors[task_id].get("floor", 4)
            if min_success_tier < current_floor:
                floors[task_id]["floor"] = min_success_tier
                logger.info(
                    "Floor lowered for %s: %d → %d",
                    task_id, current_floor, min_success_tier,
                )

        # Raise floor if we have consistent failures at a tier
        if failures and not successes:
            max_fail_tier = max(r.difficulty for r in failures)
            current_floor = floors[task_id].get("floor", 0)
            if max_fail_tier >= current_floor:
                floors[task_id]["floor"] = max_fail_tier + 1
                logger.info(
                    "Floor raised for %s: %d → %d (failures at tier %d)",
                    task_id, current_floor, max_fail_tier + 1, max_fail_tier,
                )

        # Lower ceiling if we have no need for high tiers
        if successes:
            max_success_tier = max(r.difficulty for r in successes)
            current_ceiling = floors[task_id].get("ceiling", 4)
            if max_success_tier < current_ceiling:
                floors[task_id]["ceiling"] = max_success_tier

    save_model_floors(floors, floors_path)
    return floors


# ---------------------------------------------------------------------------
# Main analysis entry points
# ---------------------------------------------------------------------------


def analyze_scores(
    *,
    db_path: str | Path | None = None,
    project: str | None = None,
    proposals_log: str | Path | None = None,
    cwd: str | None = None,
) -> list[Proposal]:
    """Standalone entry point for score-only analysis.

    Reads from task_scores SQLite, runs score classifiers, writes proposals.

    Args:
        db_path: Path to observability SQLite DB.
        project: Filter scores to this project.
        proposals_log: Path to proposals.jsonl (appended to).
        cwd: Working directory for git operations.

    Returns:
        List of proposals from score analysis.
    """
    if db_path is None:
        db_path = _DEFAULT_DB_PATH
    db_path = Path(db_path)

    if proposals_log is None:
        proposals_log = Path.home() / "projects" / "data" / "task_graph" / "proposals.jsonl"
    else:
        proposals_log = Path(proposals_log)

    scores_by_task = _load_scores_by_task(db_path, project=project)
    proposals = _generate_score_proposals(scores_by_task, cwd=cwd)

    _append_proposals(proposals_log, proposals)
    return proposals


def analyze_run(
    report: ExecutionReport,
    *,
    experiment_log: str | Path | None = None,
    proposals_log: str | Path | None = None,
    floors_path: str | Path | None = None,
    db_path: str | Path | None = None,
) -> AnalysisReport:
    """Analyze a completed graph run and generate improvement proposals.

    Args:
        report: ExecutionReport from run_graph().
        experiment_log: Path to experiments.jsonl (for historical context).
        proposals_log: Path to proposals.jsonl (appended to).
        floors_path: Path to model_floors.json.
        db_path: Path to observability SQLite DB for score analysis.

    Returns:
        AnalysisReport with proposals and updated floors.
    """
    if experiment_log is None:
        experiment_log = Path.home() / "projects" / "data" / "task_graph" / "experiments.jsonl"
    else:
        experiment_log = Path(experiment_log)
    if proposals_log is None:
        proposals_log = Path.home() / "projects" / "data" / "task_graph" / "proposals.jsonl"
    else:
        proposals_log = Path(proposals_log)

    # Load full history for context
    all_experiments = _load_experiments(experiment_log)
    by_task = _group_by_task(all_experiments)

    proposals = _generate_proposals(by_task)
    floors = _update_floors(by_task, floors_path)

    # Score-based proposals
    score_proposals: list[Proposal] = []
    effective_db = Path(db_path) if db_path else _DEFAULT_DB_PATH
    if effective_db.exists():
        scores_by_task = _load_scores_by_task(effective_db)
        score_proposals = _generate_score_proposals(scores_by_task)
        proposals.extend(score_proposals)

    # Write proposals
    _append_proposals(proposals_log, proposals)

    return AnalysisReport(
        experiments_analyzed=len(all_experiments),
        tasks_analyzed=len(by_task),
        proposals=proposals,
        floors_updated=floors,
        reliability_checks=[],
        score_proposals_count=len(score_proposals),
    )


def analyze_history(
    *,
    experiment_log: str | Path | None = None,
    proposals_log: str | Path | None = None,
    floors_path: str | Path | None = None,
    db_path: str | Path | None = None,
) -> AnalysisReport:
    """Standalone analysis of historical experiment logs.

    Same as analyze_run but doesn't need an ExecutionReport —
    reads everything from the experiment log.

    Args:
        experiment_log: Path to experiments.jsonl.
        proposals_log: Path to proposals.jsonl (appended to).
        floors_path: Path to model_floors.json.
        db_path: Path to observability SQLite DB for score analysis.
    """
    if experiment_log is None:
        experiment_log = Path.home() / "projects" / "data" / "task_graph" / "experiments.jsonl"
    else:
        experiment_log = Path(experiment_log)
    if proposals_log is None:
        proposals_log = Path.home() / "projects" / "data" / "task_graph" / "proposals.jsonl"
    else:
        proposals_log = Path(proposals_log)

    all_experiments = _load_experiments(experiment_log)
    by_task = _group_by_task(all_experiments)

    proposals = _generate_proposals(by_task)
    floors = _update_floors(by_task, floors_path)

    # Score-based proposals
    score_proposals: list[Proposal] = []
    effective_db = Path(db_path) if db_path else _DEFAULT_DB_PATH
    if effective_db.exists():
        scores_by_task = _load_scores_by_task(effective_db)
        score_proposals = _generate_score_proposals(scores_by_task)
        proposals.extend(score_proposals)

    _append_proposals(proposals_log, proposals)

    return AnalysisReport(
        experiments_analyzed=len(all_experiments),
        tasks_analyzed=len(by_task),
        proposals=proposals,
        floors_updated=floors,
        reliability_checks=[],
        score_proposals_count=len(score_proposals),
    )


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------


def _generate_proposals(
    by_task: dict[str, list[ExperimentRecord]],
) -> list[Proposal]:
    """Run all classifiers and collect proposals."""
    proposals: list[Proposal] = []
    for task_id, runs in by_task.items():
        for checker in (
            _check_model_overkill,
            _check_model_underkill,
            _check_stuck_loop,
            _check_validation_noise,
        ):
            proposal = checker(task_id, runs)
            if proposal is not None:
                proposals.append(proposal)
    return proposals


def _append_proposals(path: Path, proposals: list[Proposal]) -> None:
    """Append proposals to JSONL. Never raises."""
    if not proposals:
        return
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "a") as f:
            for p in proposals:
                f.write(p.model_dump_json() + "\n")
    except Exception:
        logger.warning("Failed to write proposals", exc_info=True)


_proposal_counter = 0


def _make_id(category_short: str, task_id: str) -> str:
    """Generate a unique proposal ID."""
    global _proposal_counter  # noqa: PLW0603
    _proposal_counter += 1
    ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    return f"prop_{ts}_{category_short}_{task_id}_{_proposal_counter}"


def _now() -> str:
    return datetime.now(timezone.utc).isoformat()
