"""Self-improvement analyzer for task graph runs.

Reads experiment JSONL logs, classifies issues into 8 categories, generates
improvement proposals with risk classification, and maintains model_floors.json
for cumulative learning.

Usage:
    from llm_client.analyzer import analyze_run, analyze_history

    # After a graph run
    proposals = analyze_run(report)

    # Standalone analysis of historical logs
    proposals = analyze_history(experiment_log="path/to/experiments.jsonl")

See docs/TASK_GRAPH_DESIGN.md for the full failure taxonomy and proposal format.
"""

from __future__ import annotations

import json
import logging
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from pydantic import BaseModel

from llm_client.difficulty import (
    get_model_for_difficulty,
    load_model_floors,
    save_model_floors,
)
from llm_client.task_graph import ExecutionReport, ExperimentRecord, TaskStatus
from llm_client.validators import ValidationResult, run_validators

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data models
# ---------------------------------------------------------------------------


class IssueCategory:
    """8-category failure taxonomy."""

    MODEL_OVERKILL = "MODEL_OVERKILL"
    MODEL_UNDERKILL = "MODEL_UNDERKILL"
    PROMPT_DRIFT = "PROMPT_DRIFT"
    VALIDATION_NOISE = "VALIDATION_NOISE"
    TOOL_GAP = "TOOL_GAP"
    STUCK_LOOP = "STUCK_LOOP"
    DATA_QUALITY = "DATA_QUALITY"
    MEASUREMENT_ERROR = "MEASUREMENT_ERROR"


class Proposal(BaseModel):
    """An improvement proposal generated by the analyzer."""

    proposal_id: str
    timestamp: str
    category: str
    task_id: str
    graph_id: str
    evidence: dict[str, Any]
    risk: str  # "low" | "medium" | "high"
    action: str
    auto_apply: bool
    applied: bool = False
    result: str | None = None


class AnalysisReport(BaseModel):
    """Summary of an analysis run."""

    experiments_analyzed: int
    tasks_analyzed: int
    proposals: list[Proposal]
    floors_updated: dict[str, dict[str, Any]]
    reliability_checks: list[dict[str, Any]]


# ---------------------------------------------------------------------------
# Experiment log reading
# ---------------------------------------------------------------------------


def _load_experiments(path: Path) -> list[ExperimentRecord]:
    """Load experiment records from JSONL file."""
    if not path.exists():
        return []
    records: list[ExperimentRecord] = []
    for line in path.read_text().splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            records.append(ExperimentRecord(**json.loads(line)))
        except Exception:
            logger.debug("Skipping malformed experiment record", exc_info=True)
    return records


def _group_by_task(
    records: list[ExperimentRecord],
) -> dict[str, list[ExperimentRecord]]:
    """Group experiments by task_id, ordered by timestamp."""
    groups: dict[str, list[ExperimentRecord]] = defaultdict(list)
    for r in records:
        groups[r.task_id].append(r)
    for task_id in groups:
        groups[task_id].sort(key=lambda r: r.timestamp)
    return dict(groups)


# ---------------------------------------------------------------------------
# Issue classifiers
# ---------------------------------------------------------------------------

# Minimum consecutive successes before proposing a downgrade
_OVERKILL_THRESHOLD = 5


def _check_model_overkill(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks that consistently succeed — candidate for cheaper model."""
    recent = runs[-_OVERKILL_THRESHOLD:]
    if len(recent) < _OVERKILL_THRESHOLD:
        return None

    # All must be successes at the same tier
    if not all(r.outcome == "confirmed" for r in recent):
        return None

    current_tier = recent[-1].difficulty
    if current_tier <= 0:
        return None  # Can't go lower than 0

    proposed_tier = current_tier - 1
    try:
        proposed_model = get_model_for_difficulty(proposed_tier, available_only=False)
    except (ValueError, RuntimeError):
        return None

    avg_cost = sum(
        r.result.get("cost_usd", 0) for r in recent
    ) / len(recent)

    graph_id = recent[-1].run_id.rsplit("_", 1)[0] if "_" in recent[-1].run_id else recent[-1].run_id

    return Proposal(
        proposal_id=_make_id("overkill", task_id),
        timestamp=_now(),
        category=IssueCategory.MODEL_OVERKILL,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "runs_analyzed": len(recent),
            "success_rate_at_current_tier": 1.0,
            "current_tier": current_tier,
            "current_model": recent[-1].model_selected,
            "proposed_tier": proposed_tier,
            "proposed_model": proposed_model,
            "estimated_savings_per_run": round(avg_cost * 0.5, 6),
        },
        risk="low",
        action="downgrade_model",
        auto_apply=True,
    )


def _check_model_underkill(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks that failed at current tier — need a stronger model."""
    if not runs:
        return None
    latest = runs[-1]
    if latest.outcome != "hypothesis_rejected":
        return None

    current_tier = latest.difficulty
    proposed_tier = current_tier + 1
    if proposed_tier > 4:
        return None  # Already at max

    try:
        proposed_model = get_model_for_difficulty(proposed_tier, available_only=False)
    except (ValueError, RuntimeError):
        return None

    graph_id = latest.run_id.rsplit("_", 1)[0] if "_" in latest.run_id else latest.run_id

    return Proposal(
        proposal_id=_make_id("underkill", task_id),
        timestamp=_now(),
        category=IssueCategory.MODEL_UNDERKILL,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "current_tier": current_tier,
            "current_model": latest.model_selected,
            "proposed_tier": proposed_tier,
            "proposed_model": proposed_model,
            "failure_reason": latest.result.get("validation_results", []),
        },
        risk="high",
        action="upgrade_model",
        auto_apply=False,  # Upgrades need human approval
    )


def _check_stuck_loop(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks that timed out or produced errors (not validation failures)."""
    if not runs:
        return None
    latest = runs[-1]
    if latest.outcome != "error":
        return None

    # Count consecutive errors
    error_streak = 0
    for r in reversed(runs):
        if r.outcome == "error":
            error_streak += 1
        else:
            break

    if error_streak < 2:
        return None  # Single error could be transient

    graph_id = latest.run_id.rsplit("_", 1)[0] if "_" in latest.run_id else latest.run_id

    return Proposal(
        proposal_id=_make_id("stuck", task_id),
        timestamp=_now(),
        category=IssueCategory.STUCK_LOOP,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "consecutive_errors": error_streak,
            "latest_error": str(latest.result.get("validation_results", "unknown")),
            "duration_s": latest.result.get("duration_s", 0),
        },
        risk="medium",
        action="reduce_timeout_or_add_stop_conditions",
        auto_apply=False,
    )


def _check_validation_noise(
    task_id: str,
    runs: list[ExperimentRecord],
) -> Proposal | None:
    """Detect tasks with inconsistent validation results across runs."""
    if len(runs) < 4:
        return None

    recent = runs[-4:]
    outcomes = [r.outcome for r in recent]

    # Look for alternating pass/fail pattern: at least 2 passes and 2 fails
    passes = outcomes.count("confirmed")
    fails = outcomes.count("hypothesis_rejected")

    if passes < 2 or fails < 2:
        return None

    graph_id = recent[-1].run_id.rsplit("_", 1)[0] if "_" in recent[-1].run_id else recent[-1].run_id

    return Proposal(
        proposal_id=_make_id("noise", task_id),
        timestamp=_now(),
        category=IssueCategory.VALIDATION_NOISE,
        task_id=task_id,
        graph_id=graph_id,
        evidence={
            "runs_analyzed": len(recent),
            "pass_count": passes,
            "fail_count": fails,
            "pattern": outcomes,
        },
        risk="medium",
        action="investigate_validator_stability",
        auto_apply=False,
    )


# ---------------------------------------------------------------------------
# Scorer reliability check
# ---------------------------------------------------------------------------


def check_scorer_reliability(
    task_id: str,
    validator_configs: list[dict[str, Any]],
) -> dict[str, Any]:
    """Re-run validators on existing outputs to check measurement stability.

    Args:
        task_id: The task being checked.
        validator_configs: The validator configs from the task definition.

    Returns:
        Dict with task_id, stable (bool), results, and any discrepancies.
    """
    if not validator_configs:
        return {"task_id": task_id, "stable": True, "results": [], "note": "no validators"}

    # Run validators twice
    run1 = run_validators(validator_configs)
    run2 = run_validators(validator_configs)

    discrepancies: list[dict[str, Any]] = []
    for i, (r1, r2) in enumerate(zip(run1, run2)):
        if r1.passed != r2.passed:
            discrepancies.append({
                "validator_index": i,
                "type": r1.type,
                "run1_passed": r1.passed,
                "run2_passed": r2.passed,
                "run1_value": r1.value,
                "run2_value": r2.value,
            })

    return {
        "task_id": task_id,
        "stable": len(discrepancies) == 0,
        "results": [r.model_dump() for r in run1],
        "discrepancies": discrepancies,
    }


# ---------------------------------------------------------------------------
# Model floors maintenance
# ---------------------------------------------------------------------------


def _update_floors(
    experiments: dict[str, list[ExperimentRecord]],
    floors_path: Path | None = None,
) -> dict[str, dict[str, Any]]:
    """Update model_floors.json based on experiment results.

    Returns the updated floors dict.
    """
    floors = load_model_floors(floors_path)
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    for task_id, runs in experiments.items():
        if not runs:
            continue

        successes = [r for r in runs if r.outcome == "confirmed"]
        failures = [r for r in runs if r.outcome in ("hypothesis_rejected", "error")]

        if task_id not in floors:
            floors[task_id] = {
                "floor": runs[-1].difficulty,
                "ceiling": runs[-1].difficulty,
                "last_tested": today,
                "runs": len(runs),
            }
        else:
            floors[task_id]["runs"] = floors[task_id].get("runs", 0) + len(runs)
            floors[task_id]["last_tested"] = today

        # Lower floor if we have successes at a lower tier
        if successes:
            min_success_tier = min(r.difficulty for r in successes)
            current_floor = floors[task_id].get("floor", 4)
            if min_success_tier < current_floor:
                floors[task_id]["floor"] = min_success_tier
                logger.info(
                    "Floor lowered for %s: %d → %d",
                    task_id, current_floor, min_success_tier,
                )

        # Raise floor if we have consistent failures at a tier
        if failures and not successes:
            max_fail_tier = max(r.difficulty for r in failures)
            current_floor = floors[task_id].get("floor", 0)
            if max_fail_tier >= current_floor:
                floors[task_id]["floor"] = max_fail_tier + 1
                logger.info(
                    "Floor raised for %s: %d → %d (failures at tier %d)",
                    task_id, current_floor, max_fail_tier + 1, max_fail_tier,
                )

        # Lower ceiling if we have no need for high tiers
        if successes:
            max_success_tier = max(r.difficulty for r in successes)
            current_ceiling = floors[task_id].get("ceiling", 4)
            if max_success_tier < current_ceiling:
                floors[task_id]["ceiling"] = max_success_tier

    save_model_floors(floors, floors_path)
    return floors


# ---------------------------------------------------------------------------
# Main analysis entry points
# ---------------------------------------------------------------------------


def analyze_run(
    report: ExecutionReport,
    *,
    experiment_log: str | Path | None = None,
    proposals_log: str | Path | None = None,
    floors_path: str | Path | None = None,
) -> AnalysisReport:
    """Analyze a completed graph run and generate improvement proposals.

    Args:
        report: ExecutionReport from run_graph().
        experiment_log: Path to experiments.jsonl (for historical context).
        proposals_log: Path to proposals.jsonl (appended to).
        floors_path: Path to model_floors.json.

    Returns:
        AnalysisReport with proposals and updated floors.
    """
    if experiment_log is None:
        experiment_log = Path.home() / "projects" / "data" / "task_graph" / "experiments.jsonl"
    else:
        experiment_log = Path(experiment_log)
    if proposals_log is None:
        proposals_log = Path.home() / "projects" / "data" / "task_graph" / "proposals.jsonl"
    else:
        proposals_log = Path(proposals_log)

    # Load full history for context
    all_experiments = _load_experiments(experiment_log)
    by_task = _group_by_task(all_experiments)

    proposals = _generate_proposals(by_task)
    floors = _update_floors(by_task, floors_path)

    # Write proposals
    _append_proposals(proposals_log, proposals)

    return AnalysisReport(
        experiments_analyzed=len(all_experiments),
        tasks_analyzed=len(by_task),
        proposals=proposals,
        floors_updated=floors,
        reliability_checks=[],
    )


def analyze_history(
    *,
    experiment_log: str | Path | None = None,
    proposals_log: str | Path | None = None,
    floors_path: str | Path | None = None,
) -> AnalysisReport:
    """Standalone analysis of historical experiment logs.

    Same as analyze_run but doesn't need an ExecutionReport —
    reads everything from the experiment log.
    """
    if experiment_log is None:
        experiment_log = Path.home() / "projects" / "data" / "task_graph" / "experiments.jsonl"
    else:
        experiment_log = Path(experiment_log)
    if proposals_log is None:
        proposals_log = Path.home() / "projects" / "data" / "task_graph" / "proposals.jsonl"
    else:
        proposals_log = Path(proposals_log)

    all_experiments = _load_experiments(experiment_log)
    by_task = _group_by_task(all_experiments)

    proposals = _generate_proposals(by_task)
    floors = _update_floors(by_task, floors_path)

    _append_proposals(proposals_log, proposals)

    return AnalysisReport(
        experiments_analyzed=len(all_experiments),
        tasks_analyzed=len(by_task),
        proposals=proposals,
        floors_updated=floors,
        reliability_checks=[],
    )


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------


def _generate_proposals(
    by_task: dict[str, list[ExperimentRecord]],
) -> list[Proposal]:
    """Run all classifiers and collect proposals."""
    proposals: list[Proposal] = []
    for task_id, runs in by_task.items():
        for checker in (
            _check_model_overkill,
            _check_model_underkill,
            _check_stuck_loop,
            _check_validation_noise,
        ):
            proposal = checker(task_id, runs)
            if proposal is not None:
                proposals.append(proposal)
    return proposals


def _append_proposals(path: Path, proposals: list[Proposal]) -> None:
    """Append proposals to JSONL. Never raises."""
    if not proposals:
        return
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "a") as f:
            for p in proposals:
                f.write(p.model_dump_json() + "\n")
    except Exception:
        logger.warning("Failed to write proposals", exc_info=True)


_proposal_counter = 0


def _make_id(category_short: str, task_id: str) -> str:
    """Generate a unique proposal ID."""
    global _proposal_counter  # noqa: PLW0603
    _proposal_counter += 1
    ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    return f"prop_{ts}_{category_short}_{task_id}_{_proposal_counter}"


def _now() -> str:
    return datetime.now(timezone.utc).isoformat()
